{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # v2 \n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "データをつくる\n",
    "X(input) -> t(teach)\n",
    "\"\"\"\n",
    "def create_data(nb_of_samples, sequence_len):\n",
    "    X = np.zeros((nb_of_samples, sequence_len))\n",
    "    for row_idx in range(nb_of_samples):\n",
    "        X[row_idx,:] = np.around(np.random.rand(sequence_len)).astype(int)\n",
    "    # Create the targets for each sequence\n",
    "    t = np.sum(X, axis=1)\n",
    "    return X, t\n",
    "\n",
    "def make_prediction(nb_of_samples,sequence_len):\n",
    "    xs, ts = create_data(nb_of_samples, sequence_len)\n",
    "    return np.array([[[y] for y in x] for x in xs]), np.array([[x] for x in ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.truncated_normal:±2σの切断正規分布からランダムに取り出したテンソルを生成する\n",
    "    - shape: 戻り値のテンソルの次元 (1次元のintegerリストもしくはテンソルを指定）\n",
    "    - mean: 生成する切断正規分布の平均値\n",
    "    - stddev: 生成する切断正規分布の標準偏差\n",
    "    - dtype: 戻り値のテンソルの型\n",
    "    - seed: integerを指定すると、sessionが変わっても都度同じ値を抽出するようになる\n",
    "    - name: operationの名前\n",
    "- inference\n",
    "    - 仮に[a-z]{4}の文字列から次の[a-z]{1}を予測する場合、\n",
    "        - chunk_size:4+1->5\n",
    "        - data_size:26 -> このままinput_layer_sizeになる\n",
    "- tf.reshape\n",
    "    - もとの形をR*C = Nとして、[-1,2]ならば−1の部分はN/2が自動代入\n",
    "- tf.transpose\n",
    "    - 転置。順序を指定して変更\n",
    "- tf.matmul\n",
    "    - (w, x)はwとxの行列積\n",
    "- tf.split(V,n,axis)\n",
    "    - Vをnで分割.axisで指定した次元で分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "実際のネットワークの構成を記述する部分\n",
    "\"\"\"\n",
    "def inference(input_ph, istate_ph):\n",
    "    \"\"\"\n",
    "    input_data: (batch_size, length_of_sequences, num_of_input_nodes = vocabulary_size) 次元のテンソル\n",
    "    initial_state: (batch_size, hidden_layer_size) 次元の行列\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "\n",
    "        # 重みとバイアスの初期化。\n",
    "        weight1_var = tf.Variable(tf.random.truncated_normal([num_of_input_nodes, num_of_hidden_nodes], stddev=0.1), name=\"weight1\")\n",
    "        weight2_var = tf.Variable(tf.random.truncated_normal([num_of_hidden_nodes, num_of_output_nodes], stddev=0.1), name=\"weight2\")\n",
    "        bias1_var = tf.Variable(tf.random.truncated_normal([num_of_hidden_nodes], stddev=0.1), name=\"bias1\")\n",
    "        bias2_var = tf.Variable(tf.random.truncated_normal([num_of_output_nodes], stddev=0.1), name=\"bias2\")\n",
    "\n",
    "        #### LSTMのcellに入る前の最初の部分 ####\n",
    "        # (length_of_sequences, batch_size, num_of_input_nodes=vocabulary_size)へ変換\n",
    "        in1 = tf.transpose(input_ph, [1, 0, 2]) \n",
    "        # (length_of_sequences * batch_size), num_of_input_nodes)に変換。これで入力可能な状態になる\n",
    "        in2 = tf.reshape(in1, [-1, num_of_input_nodes]) \n",
    "        # 重みWとバイアスBを適用。 \n",
    "        #(length_of_sequences* batch_size)* num_of_input_nodes)が\n",
    "        #(length_of_sequences* batch_size)* num_of_hidden_nodes)へ\n",
    "        # LSTMユニット内でのセル更新式．に相当\n",
    "        in3 = tf.matmul(in2, weight1_var) + bias1_var\n",
    "        #リストに分割\n",
    "        # (length_of_sequences * batch_size) *  num_of_hidden_nodes\n",
    "        # batch_size * length_of_sequences * num_of_hidden_nodes\n",
    "        in4 = tf.split(in3, length_of_sequences, 0) \n",
    "\n",
    "        #### LSTMのcell ####\n",
    "        # LSTMCell：中間層にnum_of_hidden_nodeだけニューロンを生成\n",
    "        # ｔｆにおけるcellは<状態と入力>で<新状態>を出力\n",
    "        # LSTMにおける選択的な取り込みと選択的な出力はここで解決　\n",
    "        cell = tf.compat.v1.nn.rnn_cell.LSTMCell(num_of_hidden_nodes, forget_bias=forget_bias, state_is_tuple=False)\n",
    "        # 指定Cellと指定istate_phを初期状態とする層を自動生成。そこにinputを入れて自動で\n",
    "        # 必要な数length_of_sequencesだけ生成\n",
    "        rnn_output, states_op = tf.compat.v1.nn.static_rnn(cell, in4, initial_state=istate_ph)\n",
    "        \n",
    "        #### LSTMのcellから出た後の部分 ####\n",
    "        # 最後に隠れ層から出力層につながる重みとバイアスを処理して終了です。\n",
    "        # 出力層はlength_of_sequences個のベクトルを出力\n",
    "        # outputs[-1] で最後の1文字だけを処理\n",
    "        output_op = tf.matmul(rnn_output[-1], weight2_var) + bias2_var\n",
    "        \n",
    "        # Add summary ops to collect data\n",
    "        w1_hist = tf.summary.histogram(\"weights1\", weight1_var)\n",
    "        w2_hist = tf.summary.histogram(\"weights2\", weight2_var)\n",
    "        b1_hist = tf.summary.histogram(\"biases1\", bias1_var)\n",
    "        b2_hist = tf.summary.histogram(\"biases2\", bias2_var)\n",
    "        output_hist = tf.summary.histogram(\"output\",  output_op)\n",
    "        results = [weight1_var, weight2_var, bias1_var,  bias2_var]\n",
    "        \n",
    "        return output_op, states_op, results\n",
    "\n",
    "\n",
    "\"\"\"    \n",
    "コストの計算\n",
    "より詳細な損失関数の下記かたは\n",
    "https://qiita.com/YudaiSadakuni/items/918b08e9c1fb497c96c3\n",
    "\"\"\"\n",
    "def loss(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        # tf.reduce_mean : 与えたリストに入っている数値の平均値\n",
    "        # 下記はsquare_errorを算出\n",
    "        square_error = tf.reduce_mean(tf.square(output_op - supervisor_ph))\n",
    "        loss_op = square_error\n",
    "        # lossをスカラーとして記録\n",
    "        tf.compat.v1.summary.scalar(\"loss\", loss_op)\n",
    "        return loss_op\n",
    "    \n",
    "\"\"\"\n",
    "新データを作成して、現時点での重みで精度を評価\n",
    "\"\"\"\n",
    "def calc_accuracy(output_op, prints=False):\n",
    "    #　別途でデータを新規作成\n",
    "    inputs, ts = make_prediction(num_of_prediction_epochs,length_of_sequences)\n",
    "    pred_dict = {\n",
    "        input_ph:  inputs,\n",
    "        supervisor_ph: ts,\n",
    "        istate_ph:    np.zeros((num_of_prediction_epochs, num_of_hidden_nodes * 2)),\n",
    "    }\n",
    "    #もう一度Sessoinを実行する。そしてoutput_opを実行し、値を返却\n",
    "    output = sess.run([output_op], feed_dict=pred_dict)\n",
    "\n",
    "    def print_result(i, p, q):\n",
    "#         [print(list(x)[0]) for x in i]\n",
    "        print(\"output: %f, correct: %d\" % (p, q))\n",
    "\n",
    "    if prints:\n",
    "        [print_result(i, p, q) for i, p, q in zip(inputs, output[0], ts)]\n",
    "\n",
    "    opt = abs(output - ts)[0]\n",
    "    #square error の sum \n",
    "    total = sum([1 if x[0] < 0.05 else 0 for x in opt])\n",
    "    #MSE\n",
    "    print(\"accuracy %f\" % (total / float(len(ts))))\n",
    "    \n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "コスト関数の最適化関数の定義(optimizer)\n",
    "optimizerに対して、最適化の対象を設定\n",
    "\"\"\"\n",
    "def training(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "        return training_op\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "各バッチで使用するデータを定義\n",
    "\"\"\"\n",
    "def get_batch(batch_size, X, t):\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([[t[r]] for r in rnum])\n",
    "    return xs, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "データの準備\n",
    "\"\"\"\n",
    "# 特徴量の次元\n",
    "num_of_input_nodes = 1\n",
    "#　隠れ層の次元\n",
    "num_of_hidden_nodes = 80\n",
    "#求めたい量の次元数\n",
    "num_of_output_nodes = 1\n",
    "## 時系列上のタイムステップ数\n",
    "length_of_sequences = 10\n",
    "#(補足)何回同一でーたを食わせるか\n",
    "num_of_training_epochs = 5000\n",
    "#バッチサイズ\n",
    "size_of_mini_batch = 100\n",
    "\n",
    "num_of_prediction_epochs = 100\n",
    "learning_rate = 0.01\n",
    "forget_bias = 0.8\n",
    "num_of_sample = 1000\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "#最適化関数の定義\n",
    "#今回は確率勾配\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "#こんかいは1次元でlength_of_sequencesの数列を作る。それをnum_of_sampleだけ作成する\n",
    "X, t = create_data(num_of_sample, length_of_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 0. 1. 0. 1. 1. 0.] 7.0 its mean sum of X[0] = t[0]\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[0],t[0],'its mean sum of X[0] = t[0]')\n",
    "print(len(t))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.placeholder\n",
    "    - プレースホルダーはデータが格納される入れ物。データは未定のままグラフを構築し、具体的な値は実行する時に与える。\n",
    "    - 型, size\n",
    "- tf.run\n",
    "    - Feed_dict\n",
    "        - placeholderとTensorに対応\n",
    "        - https://ja.stackoverflow.com/questions/37259/tensorflow%E3%81%AEfeed-dict-x-data-x-val%E3%81%AE%E6%84%8F%E5%91%B3%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x13f1afb50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "train#0, train loss: 2.575568e+01\n",
      "accuracy 0.000000\n",
      "train#100, train loss: 8.676588e-01\n",
      "train#200, train loss: 2.452996e-01\n",
      "train#300, train loss: 7.731947e-02\n",
      "train#400, train loss: 1.253954e-01\n",
      "train#500, train loss: 1.023796e-01\n",
      "accuracy 0.090000\n",
      "train#600, train loss: 1.665974e-02\n",
      "train#700, train loss: 4.783490e-02\n",
      "train#800, train loss: 7.561896e-02\n",
      "train#900, train loss: 9.487455e-02\n",
      "train#1000, train loss: 5.012555e-02\n",
      "accuracy 0.010000\n",
      "train#1100, train loss: 2.318375e-02\n",
      "train#1200, train loss: 9.734282e-03\n",
      "train#1300, train loss: 1.749895e-02\n",
      "train#1400, train loss: 3.587383e-02\n",
      "train#1500, train loss: 7.807648e-03\n",
      "accuracy 0.400000\n",
      "train#1600, train loss: 6.797987e-02\n",
      "train#1700, train loss: 1.606712e-02\n",
      "train#1800, train loss: 1.291443e-02\n",
      "train#1900, train loss: 5.111635e-02\n",
      "train#2000, train loss: 1.061938e-02\n",
      "accuracy 0.250000\n",
      "train#2100, train loss: 1.278130e-02\n",
      "train#2200, train loss: 1.422039e-02\n",
      "train#2300, train loss: 1.759337e-02\n",
      "train#2400, train loss: 3.107684e-03\n",
      "train#2500, train loss: 9.936688e-03\n",
      "accuracy 0.240000\n",
      "train#2600, train loss: 1.152522e-02\n",
      "train#2700, train loss: 2.006221e-03\n",
      "train#2800, train loss: 5.760193e-03\n",
      "train#2900, train loss: 5.730176e-03\n",
      "train#3000, train loss: 1.470018e-03\n",
      "accuracy 0.810000\n",
      "train#3100, train loss: 2.883255e-02\n",
      "train#3200, train loss: 5.954398e-03\n",
      "train#3300, train loss: 3.675935e-03\n",
      "train#3400, train loss: 2.954981e-02\n",
      "train#3500, train loss: 5.989372e-03\n",
      "accuracy 0.550000\n",
      "train#3600, train loss: 1.423540e-03\n",
      "train#3700, train loss: 1.075592e-02\n",
      "train#3800, train loss: 8.800686e-03\n",
      "train#3900, train loss: 2.687540e-03\n",
      "train#4000, train loss: 1.376710e-02\n",
      "accuracy 0.040000\n",
      "train#4100, train loss: 3.556288e-03\n",
      "train#4200, train loss: 2.393157e-03\n",
      "train#4300, train loss: 6.965282e-03\n",
      "train#4400, train loss: 4.882419e-03\n",
      "train#4500, train loss: 8.473545e-04\n",
      "accuracy 0.900000\n",
      "train#4600, train loss: 5.634672e-04\n",
      "train#4700, train loss: 2.488129e-03\n",
      "train#4800, train loss: 9.754828e-03\n",
      "train#4900, train loss: 6.168981e-03\n",
      "output: 6.001513, correct: 6\n",
      "output: 5.971682, correct: 6\n",
      "output: 0.213583, correct: 0\n",
      "output: 2.970394, correct: 3\n",
      "output: 4.968931, correct: 5\n",
      "output: 6.991060, correct: 7\n",
      "output: 2.952607, correct: 3\n",
      "output: 4.963206, correct: 5\n",
      "output: 5.993982, correct: 6\n",
      "output: 5.984793, correct: 6\n",
      "output: 4.984875, correct: 5\n",
      "output: 7.950559, correct: 8\n",
      "output: 4.977434, correct: 5\n",
      "output: 4.954246, correct: 5\n",
      "output: 3.949407, correct: 4\n",
      "output: 3.943500, correct: 4\n",
      "output: 2.963603, correct: 3\n",
      "output: 4.964667, correct: 5\n",
      "output: 2.959865, correct: 3\n",
      "output: 4.964660, correct: 5\n",
      "output: 3.951757, correct: 4\n",
      "output: 5.975293, correct: 6\n",
      "output: 3.955269, correct: 4\n",
      "output: 3.999188, correct: 4\n",
      "output: 5.996319, correct: 6\n",
      "output: 7.958418, correct: 8\n",
      "output: 3.957576, correct: 4\n",
      "output: 4.965918, correct: 5\n",
      "output: 2.959958, correct: 3\n",
      "output: 4.984962, correct: 5\n",
      "output: 7.958418, correct: 8\n",
      "output: 5.998602, correct: 6\n",
      "output: 3.983080, correct: 4\n",
      "output: 4.951225, correct: 5\n",
      "output: 6.981727, correct: 7\n",
      "output: 5.973143, correct: 6\n",
      "output: 3.955350, correct: 4\n",
      "output: 4.974964, correct: 5\n",
      "output: 2.970204, correct: 3\n",
      "output: 7.888064, correct: 8\n",
      "output: 7.892690, correct: 8\n",
      "output: 6.976034, correct: 7\n",
      "output: 6.072134, correct: 6\n",
      "output: 1.994705, correct: 2\n",
      "output: 2.955131, correct: 3\n",
      "output: 4.974968, correct: 5\n",
      "output: 2.967920, correct: 3\n",
      "output: 3.998541, correct: 4\n",
      "output: 6.973570, correct: 7\n",
      "output: 3.949184, correct: 4\n",
      "output: 5.972894, correct: 6\n",
      "output: 3.948562, correct: 4\n",
      "output: 8.816315, correct: 9\n",
      "output: 3.011595, correct: 3\n",
      "output: 7.959178, correct: 8\n",
      "output: 4.964660, correct: 5\n",
      "output: 3.954347, correct: 4\n",
      "output: 5.988429, correct: 6\n",
      "output: 3.957576, correct: 4\n",
      "output: 4.958135, correct: 5\n",
      "output: 5.998602, correct: 6\n",
      "output: 4.971333, correct: 5\n",
      "output: 7.004230, correct: 7\n",
      "output: 3.002255, correct: 3\n",
      "output: 5.973199, correct: 6\n",
      "output: 6.988146, correct: 7\n",
      "output: 2.960536, correct: 3\n",
      "output: 3.977779, correct: 4\n",
      "output: 3.990151, correct: 4\n",
      "output: 5.991251, correct: 6\n",
      "output: 2.976223, correct: 3\n",
      "output: 4.965253, correct: 5\n",
      "output: 3.982355, correct: 4\n",
      "output: 5.968359, correct: 6\n",
      "output: 6.975729, correct: 7\n",
      "output: 5.994655, correct: 6\n",
      "output: 5.984344, correct: 6\n",
      "output: 6.960085, correct: 7\n",
      "output: 5.989067, correct: 6\n",
      "output: 5.040545, correct: 5\n",
      "output: 2.958996, correct: 3\n",
      "output: 7.888064, correct: 8\n",
      "output: 3.973851, correct: 4\n",
      "output: 7.890860, correct: 8\n",
      "output: 3.962444, correct: 4\n",
      "output: 5.971430, correct: 6\n",
      "output: 5.974159, correct: 6\n",
      "output: 4.978384, correct: 5\n",
      "output: 5.027491, correct: 5\n",
      "output: 5.982780, correct: 6\n",
      "output: 5.985297, correct: 6\n",
      "output: 2.954096, correct: 3\n",
      "output: 7.951550, correct: 8\n",
      "output: 3.969942, correct: 4\n",
      "output: 4.974535, correct: 5\n",
      "output: 5.973553, correct: 6\n",
      "output: 7.950684, correct: 8\n",
      "output: 6.030189, correct: 6\n",
      "output: 5.977639, correct: 6\n",
      "output: 5.978627, correct: 6\n",
      "accuracy 0.890000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "tensorborsdのためにくくる\n",
    "\"\"\"\n",
    "with tf.Graph().as_default():\n",
    "    ###  変数の定義  ###\n",
    "    # input(特徴量の次元＊特徴量のタイムステップ数)\n",
    "    input_ph = tf.compat.v1.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n",
    "    # actual(求めたい量の次元数)\n",
    "    supervisor_ph = tf.compat.v1.placeholder(tf.float32, [None, num_of_output_nodes], name=\"supervisor\")\n",
    "    # hidden\n",
    "    istate_ph = tf.compat.v1.placeholder(tf.float32, [None, num_of_hidden_nodes * 2], name=\"istate\")\n",
    "    \n",
    "    ### グラフを構築(呼び出し時には実際に計算される部分)  ###\n",
    "    ### いずれも変数\n",
    "    output_op, states_op, datas_op = inference(input_ph, istate_ph)\n",
    "    ### 損失関数を定義（独自）  ###\n",
    "    ### 正確には損失を計算するoperatorを定義。runしないと動かない\n",
    "    loss_op = loss(output_op, supervisor_ph)\n",
    "     ### optimizerを定義  ###\n",
    "    ### 正確には損失を計算するoperatorを定義。runしないと動かない\n",
    "    training_op = training(loss_op)\n",
    "\n",
    "    # TensorBoard描画用の変数(Summry)\n",
    "    # TensorBoardが利用するSummaryデータをmerge（合体）する\n",
    "    summary_op = tf.compat.v1.summary.merge_all()\n",
    "    \n",
    "    # 定義した変数をここで初期化\n",
    "    init =tf.compat.v1.initialize_all_variables()\n",
    "\n",
    "    ## グラフの事項単位をセッション（Session）###\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        #Tensorflowの学習パラーメータのsave\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        # TensorBoardが利用するSummaryデータのライターを作成。\n",
    "        # 構築されたグラフを取得\n",
    "        summary_writer =tf.compat.v1.summary.FileWriter(\"./tmp/tensorflow_log\", graph=sess.graph)\n",
    "        #Sessionオブジェクトを作成し、runメソッドを呼び出して対象を評価\n",
    "        sess.run(init)\n",
    "\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            inputs, supervisors = get_batch(size_of_mini_batch, X, t)\n",
    "            # istate_ph\n",
    "            #  状態を伝達するのみ(length_of_sequences分の確保は不要)\n",
    "            #   size_of_mini_batch *  num_of_hidden_node の順番もinput に揃える\n",
    "            train_dict = {\n",
    "                input_ph:      inputs,\n",
    "                supervisor_ph: supervisors,\n",
    "                istate_ph:     np.zeros((size_of_mini_batch, num_of_hidden_nodes * 2)),\n",
    "            }\n",
    "            ## グラフ（グラフと損失関数について）とデータを装填し、実行\n",
    "            sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "            if (epoch) % 100 == 0:\n",
    "                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                print(\"train#%d, train loss: %e\" % (epoch, train_loss))\n",
    "                \n",
    "                #SummaryWriterにSummaryの評価結果を渡す\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                \n",
    "                #精度評価\n",
    "                if (epoch) % 500 == 0:\n",
    "                    calc_accuracy(output_op)\n",
    "\n",
    "        calc_accuracy(output_op, prints=True)\n",
    "        datas = sess.run(datas_op)\n",
    "        saver.save(sess, \"./data/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
