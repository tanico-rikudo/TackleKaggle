{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_categories(target_exchange):\n",
    "    '''分類クラス名とその値を計算するためのオペレーター関数を列挙する。\n",
    "    '''\n",
    "    for polarity, operator in [\n",
    "            ('positive', op.ge), # >=\n",
    "            ('negative', op.lt), # <\n",
    "    ]:\n",
    "        colname = '{}_{}'.format(target_exchange, polarity)\n",
    "        yield colname, target_exchange, operator\n",
    "\n",
    "\n",
    "def iter_exchange_days_back(target_exchange, max_days_back):\n",
    "    '''指標名、何日前のデータを読むか、カラム名を列挙する。\n",
    "    '''\n",
    "    for exchange in EXCHANGES_LABEL:\n",
    "        # SP500 の結果を予測するのに SP500 の当日の値が含まれてはいけないので１日づらす\n",
    "        start_days_back = 1 if exchange == target_exchange else 0\n",
    "        #start_days_back = 1 # N225 で行う場合は全て前日の指標を使うようにする\n",
    "        end_days_back = start_days_back + max_days_back\n",
    "        for days_back in range(start_days_back, end_days_back):\n",
    "            colname = '{}_{}'.format(exchange, days_back)\n",
    "            yield colname, exchange, days_back\n",
    "\n",
    "\n",
    "def split_training_test_data(num_categories, training_test_data):\n",
    "    '''学習データをトレーニング用とテスト用に分割する。\n",
    "    '''\n",
    "    # 先頭２つより後ろが学習データ\n",
    "    predictors_tf = training_test_data[training_test_data.columns[num_categories:]]\n",
    "    # 先頭２つが「上がる」「下がる」の答えデータ\n",
    "    classes_tf = training_test_data[training_test_data.columns[:num_categories]]\n",
    "\n",
    "    # 学習用とテスト用のデータサイズを求める\n",
    "    training_set_size = int(len(training_test_data) * 0.8)\n",
    "    test_set_size = len(training_test_data) - training_set_size\n",
    "\n",
    "    # 古いデータ0.8を学習とし、新しいデータ0.2がテストとなる\n",
    "    return Dataset(\n",
    "        training_predictors=predictors_tf[:training_set_size],\n",
    "        training_classes=classes_tf[:training_set_size],\n",
    "        test_predictors=predictors_tf[training_set_size:],\n",
    "        test_classes=classes_tf[training_set_size:],\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "    '''与えられたネットワークの正解率などを出力する。\n",
    "    '''\n",
    "    predictions = tf.argmax(model, 1)\n",
    "    actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "    tp_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, ones_like_actuals),\n",
    "                tf.equal(predictions, ones_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tn_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, zeros_like_actuals),\n",
    "                tf.equal(predictions, zeros_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fp_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, zeros_like_actuals),\n",
    "                tf.equal(predictions, ones_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fn_op = tf.reduce_sum(\n",
    "        tf.cast(\n",
    "            tf.logical_and(\n",
    "                tf.equal(actuals, ones_like_actuals),\n",
    "                tf.equal(predictions, zeros_like_predictions)\n",
    "            ),\n",
    "            \"float\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tp, tn, fp, fn = session.run(\n",
    "        [tp_op, tn_op, fp_op, fn_op],\n",
    "        feed_dict\n",
    "    )\n",
    "\n",
    "    tpr = float(tp)/(float(tp) + float(fn))\n",
    "    fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "    accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "    recall = tpr\n",
    "    if (float(tp) + float(fp)):\n",
    "        precision = float(tp)/(float(tp) + float(fp))\n",
    "        f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "    else:\n",
    "        precision = 0\n",
    "        f1_score = 0\n",
    "\n",
    "    print('Precision = ', precision)\n",
    "    print('Recall = ', recall)\n",
    "    print('F1 Score = ', f1_score)\n",
    "    print('Accuracy = ', accuracy)\n",
    "\n",
    "\n",
    "def simple_network(dataset):\n",
    "    '''単純な分類モデルを返す。\n",
    "    '''\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "    num_predictors = len(dataset.training_predictors.columns)\n",
    "    num_classes = len(dataset.training_classes.columns)\n",
    "\n",
    "    # Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "    feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "    actual_classes = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    # Define a matrix of weights and initialize it with some small random values.\n",
    "    weights = tf.Variable(tf.truncated_normal([num_predictors, num_classes], stddev=0.0001))\n",
    "    biases = tf.Variable(tf.ones([num_classes]))\n",
    "\n",
    "    # Define our model...\n",
    "    # Here we take a softmax regression of the product of our feature data and weights.\n",
    "    model = tf.nn.softmax(tf.matmul(feature_data, weights) + biases)\n",
    "\n",
    "    # Define a cost function (we're using the cross entropy).\n",
    "    cost = -tf.reduce_sum(actual_classes * tf.log(model))\n",
    "\n",
    "    # Define a training step...\n",
    "    # Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "    training_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    return Environ(\n",
    "        sess=sess,\n",
    "        model=model,\n",
    "        actual_classes=actual_classes,\n",
    "        training_step=training_step,\n",
    "        dataset=dataset,\n",
    "        feature_data=feature_data,\n",
    "    )\n",
    "\n",
    "\n",
    "def smarter_network(dataset):\n",
    "    '''隠しレイヤー入りのもうちょっと複雑な分類モデルを返す。\n",
    "    '''\n",
    "    sess = tf.Session()\n",
    "\n",
    "    num_predictors = len(dataset.training_predictors.columns)\n",
    "    num_classes = len(dataset.training_classes.columns)\n",
    "\n",
    "    feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "    actual_classes = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "    weights1 = tf.Variable(tf.truncated_normal([(DAYS_BACK * len(EXCHANGES_DEFINE)), 50], stddev=0.0001))\n",
    "    biases1 = tf.Variable(tf.ones([50]))\n",
    "\n",
    "    weights2 = tf.Variable(tf.truncated_normal([50, 25], stddev=0.0001))\n",
    "    biases2 = tf.Variable(tf.ones([25]))\n",
    "\n",
    "    weights3 = tf.Variable(tf.truncated_normal([25, 2], stddev=0.0001))\n",
    "    biases3 = tf.Variable(tf.ones([2]))\n",
    "\n",
    "    # This time we introduce a single hidden layer into our model...\n",
    "    hidden_layer_1 = tf.nn.relu(tf.matmul(feature_data, weights1) + biases1)\n",
    "    hidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, weights2) + biases2)\n",
    "    model = tf.nn.softmax(tf.matmul(hidden_layer_2, weights3) + biases3)\n",
    "\n",
    "    cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "    training_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "\n",
    "    return Environ(\n",
    "        sess=sess,\n",
    "        model=model,\n",
    "        actual_classes=actual_classes,\n",
    "        training_step=training_step,\n",
    "        dataset=dataset,\n",
    "        feature_data=feature_data,\n",
    "    )\n",
    "\n",
    "\n",
    "def train(env, steps=30000, checkin_interval=5000):\n",
    "    '''学習をsteps回おこなう。\n",
    "    '''\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(env.model, 1),\n",
    "        tf.argmax(env.actual_classes, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    for i in range(1, 1 + steps):\n",
    "        env.sess.run(\n",
    "            env.training_step,\n",
    "            feed_dict=feed_dict(env, test=False),\n",
    "        )\n",
    "        if i % checkin_interval == 0:\n",
    "            print(i, env.sess.run(\n",
    "                accuracy,\n",
    "                feed_dict=feed_dict(env, test=False),\n",
    "            ))\n",
    "\n",
    "    tf_confusion_metrics(env.model, env.actual_classes, env.sess, feed_dict(env, True))\n",
    "\n",
    "\n",
    "def feed_dict(env, test=False):\n",
    "    '''学習/テストに使うデータを生成する。\n",
    "    '''\n",
    "    prefix = 'test' if test else 'training'\n",
    "    predictors = getattr(env.dataset, '{}_predictors'.format(prefix))\n",
    "    classes = getattr(env.dataset, '{}_classes'.format(prefix))\n",
    "    return {\n",
    "        env.feature_data: predictors.values,\n",
    "        env.actual_classes: classes.values.reshape(len(classes.values), len(classes.columns))\n",
    "    }\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print('株価指標データをダウンロードしcsvファイルに保存')\n",
    "    fetchStockIndexes()\n",
    "    print('株価指標データを読み込む')\n",
    "    all_data  = load_exchange_dataframes()\n",
    "    print('終値を取得')\n",
    "    closing_data = get_closing_data(all_data)\n",
    "    print('データを学習に使える形式に正規化')\n",
    "    log_return_data = get_log_return_data(closing_data)\n",
    "    print('答と学習データを作る')\n",
    "    num_categories, training_test_data = build_training_data(\n",
    "        log_return_data, args.target_exchange,\n",
    "        use_subset=args.use_subset)\n",
    "    print('学習データをトレーニング用とテスト用に分割する')\n",
    "    dataset = split_training_test_data(num_categories, training_test_data)\n",
    "\n",
    "    print('器械学習のネットワークを作成')\n",
    "    #env = simple_network(dataset)\n",
    "    env = smarter_network(dataset)\n",
    "\n",
    "    if args.inspect:\n",
    "        import code\n",
    "        print('Press Ctrl-d to proceed')\n",
    "        code.interact(local=locals())\n",
    "\n",
    "    print('学習')\n",
    "    train(env, steps=args.steps, checkin_interval=args.checkin)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('target_exchange', choices=EXCHANGES_LABEL)\n",
    "    parser.add_argument('--steps', type=int, default=10000)\n",
    "    parser.add_argument('--checkin', type=int, default=1000)\n",
    "    parser.add_argument('--use-subset', type=float, default=None)\n",
    "    parser.add_argument('--inspect', type=bool, default=False)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code based on:\n",
    "https://github.com/corrieelston/datalab/blob/master/FinancialTimeSeriesTensorFlow.ipynb\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import urllib3\n",
    "from os import path\n",
    "import operator as op\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set value ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_BACK = 3\n",
    "FROM_YEAR = '1991'\n",
    "EXCHANGES_DEFINE = [\n",
    "    #['DOW', '^DJI'],\n",
    "    ['FTSE', '^FTSE'],\n",
    "    ['GDAXI', '^GDAXI'],\n",
    "    ['HSI', '^HSI'],\n",
    "    ['N225', '^N225'],\n",
    "    #['NASDAQ', '^IXIC'],\n",
    "    ['SP500', '^GSPC'],\n",
    "    ['SSEC', '000001.SS'],\n",
    "]\n",
    "EXCHANGES_LABEL = [exchange[0] for exchange in EXCHANGES_DEFINE]\n",
    "\n",
    "Dataset = namedtuple(\n",
    "    'Dataset',\n",
    "    'training_predictors training_classes test_predictors test_classes')\n",
    "\n",
    "Environ = namedtuple('Environ', 'sess model actual_classes training_step dataset feature_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupDateURL(urlBase):\n",
    "    now = datetime.date.today()\n",
    "    return urlBase.replace('__FROM_YEAR__', FROM_YEAR)\\\n",
    "            .replace('__TO_MONTH__', str(now.month - 1))\\\n",
    "            .replace('__TO_DAY__', str(now.day))\\\n",
    "            .replace('__TO_YEAR__', str(now.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch from url and Output to CSV\n",
    "def fetchCSV(fileName, url):\n",
    "    if path.isfile(fileName):\n",
    "        print('fetch CSV for local: ' + fileName)\n",
    "        with open(fileName) as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        print('fetch CSV for url: ' + url)\n",
    "        csv = urllib2.urlopen(url).read()\n",
    "        with open(fileName, 'w') as f:\n",
    "            f.write(csv)\n",
    "        return csv\n",
    "\n",
    "# Fetch yahoo\n",
    "def fetchYahooFinance(name, code):\n",
    "    fileName = 'index_%s.csv' % name\n",
    "    \n",
    "    #period is already set\n",
    "    url = setupDateURL('http://chart.finance.yahoo.com/table.csv?s=%s&a=0&b=1&c=__FROM_YEAR__&d=__TO_MONTH__&e=__TO_DAY__&f=__TO_YEAR__&g=d&ignore=.csv' % code)\n",
    "    csv = fetchCSV(fileName, url)\n",
    "\n",
    "# Fetch data\n",
    "def fetchStockIndexes():\n",
    "    '''株価指標のデータをダウンロードしファイルに保存\n",
    "    '''\n",
    "    for exchange in EXCHANGES_DEFINE:\n",
    "        fetchYahooFinance(exchange[0], exchange[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exchange_dataframes():\n",
    "    '''EXCHANGESに対応するCSVファイルをPandasのDataFrameとして読み込む。\n",
    "    Returns:\n",
    "        {EXCHANGES[n]: pd.DataFrame()}\n",
    "    '''\n",
    "    return {exchange: load_exchange_dataframe(exchange)\n",
    "            for exchange in EXCHANGES_LABEL}\n",
    "\n",
    "\n",
    "def load_exchange_dataframe(exchange):\n",
    "    '''exchangeに対応するCSVファイルをPandasのDataFrameとして読み込む。\n",
    "    Args:\n",
    "        exchange: 指標名\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "    return pd.read_csv('index_{}.csv'.format(exchange)).set_index('Date').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closing_data(dataframes):\n",
    "    '''各指標の終値カラムをまとめて1つのDataFrameに詰める。\n",
    "    Args:\n",
    "        dataframes: {key: pd.DataFrame()}\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "    closing_data = pd.DataFrame()\n",
    "    for exchange, dataframe in dataframes.items():\n",
    "        closing_data[exchange] = dataframe['Close']\n",
    "    closing_data = closing_data.fillna(method='ffill')\n",
    "    return closing_data\n",
    "\n",
    "\n",
    "def get_log_return_data(closing_data):\n",
    "    '''各指標について、終値を1日前との比率の対数をとって正規化する。\n",
    "    Args:\n",
    "        closing_data: pd.DataFrame()\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "\n",
    "    log_return_data = pd.DataFrame()\n",
    "    for exchange in closing_data:\n",
    "        # np.log(当日終値 / 前日終値) で前日からの変化率を算出\n",
    "        # 前日よりも上がっていればプラス、下がっていればマイナスになる\n",
    "        log_return_data[exchange] = np.log(closing_data[exchange]/closing_data[exchange].shift())\n",
    "\n",
    "    return log_return_data\n",
    "\n",
    "\n",
    "def build_training_data(log_return_data, target_exchange, max_days_back=DAYS_BACK, use_subset=None):\n",
    "    '''学習データを作る。分類クラスは、target_exchange指標の終値が前日に比べて上ったか下がったかの2つである。\n",
    "    また全指標の終値の、当日から数えてmax_days_back日前までを含めて入力データとする。\n",
    "    Args:\n",
    "        log_return_data: pd.DataFrame()\n",
    "        target_exchange: 学習目標とする指標名\n",
    "        max_days_back: 何日前までの終値を学習データに含めるか\n",
    "        use_subset (float): 短時間で動作を確認したい時用: log_return_dataのうち一部だけを学習データに含める\n",
    "    Returns:\n",
    "        pd.DataFrame()\n",
    "    '''\n",
    "    # 「上がる」「下がる」の結果を計算\n",
    "    columns = []\n",
    "    for colname, exchange, operator in iter_categories(target_exchange):\n",
    "        columns.append(colname)\n",
    "        # 全ての XXX_positive, XXX_negative を 0 に初期化\n",
    "        log_return_data[colname] = 0\n",
    "        # XXX_positive の場合は >=  0 の全てのインデックスを\n",
    "        # XXX_negative の場合は < 0 の全てのインデックスを取得し、それらに 1 を設定する\n",
    "        indices = operator(log_return_data[exchange], 0)\n",
    "        log_return_data.ix[indices, colname] = 1\n",
    "\n",
    "    num_categories = len(columns)\n",
    "\n",
    "    # 各指標のカラム名を追加\n",
    "    for colname, _, _ in iter_exchange_days_back(target_exchange, max_days_back):\n",
    "        columns.append(colname)\n",
    "\n",
    "    '''\n",
    "    columns には計算対象の positive, negative と各指標の日数分のラベルが含まれる\n",
    "    例：[\n",
    "        'SP500_positive',\n",
    "        'SP500_negative',\n",
    "        'DOW_0',\n",
    "        'DOW_1',\n",
    "        'DOW_2',\n",
    "        'FTSE_0',\n",
    "        'FTSE_1',\n",
    "        'FTSE_2',\n",
    "        'GDAXI_0',\n",
    "        'GDAXI_1',\n",
    "        'GDAXI_2',\n",
    "        'HSI_0',\n",
    "        'HSI_1',\n",
    "        'HSI_2',\n",
    "        'N225_0',\n",
    "        'N225_1',\n",
    "        'N225_2',\n",
    "        'NASDAQ_0',\n",
    "        'NASDAQ_1',\n",
    "        'NASDAQ_2',\n",
    "        'SP500_1',\n",
    "        'SP500_2',\n",
    "        'SP500_3',\n",
    "        'SSEC_0',\n",
    "        'SSEC_1',\n",
    "        'SSEC_2'\n",
    "    ]\n",
    "    計算対象の SP500 だけ当日のデータを含めたらダメなので1〜3が入る\n",
    "    '''\n",
    "\n",
    "    # データ数をもとめる\n",
    "    max_index = len(log_return_data) - max_days_back\n",
    "    if use_subset is not None:\n",
    "        # データを少なくしたいとき\n",
    "        max_index = int(max_index * use_subset)\n",
    "\n",
    "    # 学習データを作る\n",
    "    training_test_data = pd.DataFrame(columns=columns)\n",
    "    for i in range(max_days_back + 10, max_index):\n",
    "        # 先頭のデータを含めるとなぜか上手くいかないので max_days_back + 10 で少し省く\n",
    "        values = {}\n",
    "        # 「上がる」「下がる」の答を入れる\n",
    "        for colname, _, _ in iter_categories(target_exchange):\n",
    "            values[colname] = log_return_data[colname].ix[i]\n",
    "        # 学習データを入れる\n",
    "        for colname, exchange, days_back in iter_exchange_days_back(target_exchange, max_days_back):\n",
    "            values[colname] = log_return_data[exchange].ix[i - days_back]\n",
    "        training_test_data = training_test_data.append(values, ignore_index=True)\n",
    "\n",
    "    return num_categories, training_test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
