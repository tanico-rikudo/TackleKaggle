{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 4s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "print(\"Download to..:\",path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    #正規化；Normalization Form Decompose\n",
    "    #汎用カテゴリ：http://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "    #Mn: \tNonspacing_Mark\ta nonspacing combining mark (zero advance width)\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "####  sentence preprocesser\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # モデルが予測をいつ開始し、いつ終了すれば良いかを知らせるため\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 指定データセットからnum_examples文だけ(enflish , spanish)作る\n",
    "# ある程度分かち書きや正規化などはする。\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベクトル*文章数のテンソルのうち、最大次元数\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.keras.preprocessing.text.Tokenizer(\n",
    "- テキストをトークン化するユーティリティクラス．\n",
    "    - 各テキストを整数の列（各整数はは辞書におけるトークンのインデックス）または単語のカウントやtf-idfなどに基づいて各トークンをバイナリとした係数からなるベクトルに変換することで，テキストコーパスをベクトル化します．\n",
    "- テキストをベクトル化する，または/かつ，テキストをシーケンス（= データセット中でランクi（1から始まる）の単語がインデックスiを持つ単語インデックスのリスト）に変換するクラス．\n",
    "- args\n",
    "    - num_words: 利用する単語の最大数で単語の頻度に基づきます．一般的にはnum_words-1が用いられます．\n",
    "    - filters: テキストからフィルタする文字の要素からなる文字列．デフォルトでは全ての句読点に加えてタブや開業，マイナス，'文字です．\n",
    "    - lower: 真理値．テキストを小文字にするかどうか．\n",
    "    - split: 文字列．単語を分割するセパレータ．\n",
    "    - char_level: Trueなら，全文字はトークンとして扱われます．\n",
    "    - oov_token: 与えられた場合，単語のインデックスに付与され，text_to_sequenceが呼ばれた時に語彙にない単語を入れ替えるために使われます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### langに対して、ベクトル化\n",
    "# そのままtensor\n",
    "# 固定長lang_tokenizer\n",
    "\n",
    "def tokenize(lang):\n",
    "    # Tokenizerをインスタンス化\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    # 文章を与える。\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "#     print(\"#doc : \", lang_tokenizer.document_count)\n",
    "#     print(\"#words in doc : \\n\", lang_tokenizer.word_counts)\n",
    "#     print(\"index of word :\",lang_tokenizer.word_index)\n",
    "\n",
    "    #単語のシーケンス番号（1～）の列を示すベクトルが得られる。\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "#     print(tensor)\n",
    "\n",
    "    #固定長に変換\n",
    "    #最大長指定なし→最大の文字長に合わせる\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#doc :  280\n",
      "#words in doc : \n",
      " OrderedDict([('<', 2), ('s', 16), ('t', 15), ('a', 30), ('r', 16), ('>', 2), ('i', 13), ('q', 3), ('u', 10), ('e', 29), ('o', 17), ('n', 15), ('c', 10), ('m', 11), ('h', 2), ('b', 3), ('l', 6), ('v', 3), (',', 1), ('d', 9), ('p', 6), ('f', 2), ('y', 3), ('z', 2), ('j', 1), ('.', 1)])\n",
      "[[19], [4], [6], [1], [5], [6], [20], [], [4], [8], [], [15], [10], [8], [2], [5], [2], [4], [], [4], [3], [7], [1], [5], [], [11], [3], [9], [3], [], [10], [7], [], [21], [1], [16], [13], [1], [7], [6], [2], [], [7], [1], [6], [8], [17], [3], [], [24], [], [12], [2], [16], [2], [4], [], [2], [4], [6], [1], [5], [], [12], [8], [4], [14], [10], [2], [4], [6], [3], [], [1], [], [14], [5], [1], [11], [6], [8], [11], [1], [5], [], [12], [8], [11], [8], [2], [7], [12], [3], [], [13], [1], [], [9], [8], [4], [9], [1], [], [22], [5], [1], [4], [2], [], [10], [7], [1], [], [18], [], [3], [6], [5], [1], [], [17], [2], [23], [], [12], [2], [], [13], [1], [], [9], [8], [4], [9], [1], [], [9], [1], [7], [2], [5], [1], [], [2], [7], [], [15], [10], [2], [], [10], [7], [], [9], [10], [4], [8], [11], [3], [], [12], [2], [], [16], [1], [7], [25], [3], [], [14], [5], [1], [11], [6], [8], [11], [1], [], [2], [13], [], [9], [8], [4], [9], [3], [], [22], [5], [1], [4], [2], [3], [], [10], [7], [1], [], [18], [], [3], [6], [5], [1], [], [17], [2], [23], [], [21], [1], [4], [6], [1], [], [15], [10], [2], [], [13], [3], [], [14], [10], [2], [12], [1], [7], [], [6], [3], [11], [1], [5], [], [11], [3], [5], [5], [2], [11], [6], [1], [9], [2], [7], [6], [2], [], [18], [], [2], [7], [], [2], [13], [], [6], [8], [2], [9], [14], [3], [], [2], [4], [14], [2], [5], [1], [12], [3], [], [26], [], [19], [2], [7], [12], [20]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[19],\n",
       "        [ 4],\n",
       "        [ 6],\n",
       "        [ 1],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [20],\n",
       "        [ 0],\n",
       "        [ 4],\n",
       "        [ 8],\n",
       "        [ 0],\n",
       "        [15],\n",
       "        [10],\n",
       "        [ 8],\n",
       "        [ 2],\n",
       "        [ 5],\n",
       "        [ 2],\n",
       "        [ 4],\n",
       "        [ 0],\n",
       "        [ 4],\n",
       "        [ 3],\n",
       "        [ 7],\n",
       "        [ 1],\n",
       "        [ 5],\n",
       "        [ 0],\n",
       "        [11],\n",
       "        [ 3],\n",
       "        [ 9],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [10],\n",
       "        [ 7],\n",
       "        [ 0],\n",
       "        [21],\n",
       "        [ 1],\n",
       "        [16],\n",
       "        [13],\n",
       "        [ 1],\n",
       "        [ 7],\n",
       "        [ 6],\n",
       "        [ 2],\n",
       "        [ 0],\n",
       "        [ 7],\n",
       "        [ 1],\n",
       "        [ 6],\n",
       "        [ 8],\n",
       "        [17],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [24],\n",
       "        [ 0],\n",
       "        [12],\n",
       "        [ 2],\n",
       "        [16],\n",
       "        [ 2],\n",
       "        [ 4],\n",
       "        [ 0],\n",
       "        [ 2],\n",
       "        [ 4],\n",
       "        [ 6],\n",
       "        [ 1],\n",
       "        [ 5],\n",
       "        [ 0],\n",
       "        [12],\n",
       "        [ 8],\n",
       "        [ 4],\n",
       "        [14],\n",
       "        [10],\n",
       "        [ 2],\n",
       "        [ 4],\n",
       "        [ 6],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [14],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [11],\n",
       "        [ 6],\n",
       "        [ 8],\n",
       "        [11],\n",
       "        [ 1],\n",
       "        [ 5],\n",
       "        [ 0],\n",
       "        [12],\n",
       "        [ 8],\n",
       "        [11],\n",
       "        [ 8],\n",
       "        [ 2],\n",
       "        [ 7],\n",
       "        [12],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [13],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [ 8],\n",
       "        [ 4],\n",
       "        [ 9],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [22],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [ 4],\n",
       "        [ 2],\n",
       "        [ 0],\n",
       "        [10],\n",
       "        [ 7],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 3],\n",
       "        [ 6],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [17],\n",
       "        [ 2],\n",
       "        [23],\n",
       "        [ 0],\n",
       "        [12],\n",
       "        [ 2],\n",
       "        [ 0],\n",
       "        [13],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [ 8],\n",
       "        [ 4],\n",
       "        [ 9],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [ 1],\n",
       "        [ 7],\n",
       "        [ 2],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [ 2],\n",
       "        [ 7],\n",
       "        [ 0],\n",
       "        [15],\n",
       "        [10],\n",
       "        [ 2],\n",
       "        [ 0],\n",
       "        [10],\n",
       "        [ 7],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [ 4],\n",
       "        [ 8],\n",
       "        [11],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [12],\n",
       "        [ 2],\n",
       "        [ 0],\n",
       "        [16],\n",
       "        [ 1],\n",
       "        [ 7],\n",
       "        [25],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [14],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [11],\n",
       "        [ 6],\n",
       "        [ 8],\n",
       "        [11],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [ 2],\n",
       "        [13],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [ 8],\n",
       "        [ 4],\n",
       "        [ 9],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [22],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [ 4],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [10],\n",
       "        [ 7],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 3],\n",
       "        [ 6],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [17],\n",
       "        [ 2],\n",
       "        [23],\n",
       "        [ 0],\n",
       "        [21],\n",
       "        [ 1],\n",
       "        [ 4],\n",
       "        [ 6],\n",
       "        [ 1],\n",
       "        [ 0],\n",
       "        [15],\n",
       "        [10],\n",
       "        [ 2],\n",
       "        [ 0],\n",
       "        [13],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [14],\n",
       "        [10],\n",
       "        [ 2],\n",
       "        [12],\n",
       "        [ 1],\n",
       "        [ 7],\n",
       "        [ 0],\n",
       "        [ 6],\n",
       "        [ 3],\n",
       "        [11],\n",
       "        [ 1],\n",
       "        [ 5],\n",
       "        [ 0],\n",
       "        [11],\n",
       "        [ 3],\n",
       "        [ 5],\n",
       "        [ 5],\n",
       "        [ 2],\n",
       "        [11],\n",
       "        [ 6],\n",
       "        [ 1],\n",
       "        [ 9],\n",
       "        [ 2],\n",
       "        [ 7],\n",
       "        [ 6],\n",
       "        [ 2],\n",
       "        [ 0],\n",
       "        [18],\n",
       "        [ 0],\n",
       "        [ 2],\n",
       "        [ 7],\n",
       "        [ 0],\n",
       "        [ 2],\n",
       "        [13],\n",
       "        [ 0],\n",
       "        [ 6],\n",
       "        [ 8],\n",
       "        [ 2],\n",
       "        [ 9],\n",
       "        [14],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [ 2],\n",
       "        [ 4],\n",
       "        [14],\n",
       "        [ 2],\n",
       "        [ 5],\n",
       "        [ 1],\n",
       "        [12],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [26],\n",
       "        [ 0],\n",
       "        [19],\n",
       "        [ 2],\n",
       "        [ 7],\n",
       "        [12],\n",
       "        [20]], dtype=int32),\n",
       " <keras_preprocessing.text.Tokenizer at 0x131247510>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # creating data\n",
    "    # get input and clean it , output pair\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    #to tensor \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "6575 ----> inspira\n",
      "1409 ----> profundamente\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "1060 ----> breathe\n",
      "33 ----> in\n",
      "2201 ----> deeply\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "##　ちゃんと対応取れているよ！\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "#埋め込み次元\n",
    "embedding_dim = 256\n",
    "#Encoderのユニット数(隠れ層のユニット数)\n",
    "units = 1024\n",
    "\n",
    "#入力の辞書サイズ\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "#ターゲットの辞書サイズ\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "#文字列の配列をスライスすると、文字列のデータセットが出来上がります。\n",
    "#シャッフルで\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "#バッチサイズに区切る\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoderの設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#エンコーダーは入力文字が各ユニットにどの程度重みをつけるかを計算する。\n",
    "class Encoder(tf.keras.Model):\n",
    "    #vocab_size: 辞書サイズ,\n",
    "    #embedding_dim: 隠れ層サイズ\n",
    "    #enc_units: encodeのユニット数\n",
    "    #batch_sz: バッチサイズ\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        ## GRU:RNN以上LSTM以下のもの\n",
    "        ## Gated Recurrent Unit\n",
    "        # Glorot の正規分布（Xavier の正規分布とも呼ばれます）による初期化を実施\n",
    "        # https://keras.io/ja/initializers/\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    #フォーワード計算を行う。\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    # 残りの初期化を行う \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# サンプル入力\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "# 試しに１バッチだけ入れてみる\n",
    "# Call呼び出し\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, hidden units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, hidden units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attensionの設置 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    #units:コンテキストベクターとしてデコーダに渡すので、decodeerのunit数\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # スコアを計算するためにこのように加算を実行する\n",
    "        #tf.expand_dims　添字\"axis\"でのサイズ1の次元を加える\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # スコアを self.V に適用するために最後の軸は 1 となる        \n",
    "        # self.V に適用する前のテンソルの shape は  (batch_size, max_length, units)\n",
    "        # Denceは和を計算(1次元)\n",
    "        # ==> Scoreは関連度；　推測したい言葉(target)から計算された出力(query：hidden_with_time_axis)を全てのencoder出力(values)から計算される。\n",
    "        # transformerなら内積取るらしい\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(values) + self.W2(hidden_with_time_axis)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # attention_weights の shape == (batch_size, max_length, 1)\n",
    "        # 各max_length分のユニットに対し、attention_weightsが作成される(batch_size分)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector の合計後の shape == (batch_size, hidden_size)\n",
    "        # 関連度でencoderに重みつける(values:全てのencoder出力)\n",
    "        context_vector = attention_weights * values\n",
    "        \n",
    "        #　axis+1次元目で総和を取る\n",
    "        # 全てのencoder出力と推測したい言葉(target)から計算された出力(状態)を鑑みた結果が出てくる\n",
    "        #　つまり ｑuery と　valueで毎回全結合層のアップデートが掛かる＝＞データのどこにattentionを向けるのかが変化\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "#サンプル\n",
    "attention_layer = BahdanauAttention(10)\n",
    "#encodeerのテストで作成済み\n",
    "# sample_hidden:隠れ層の状態\n",
    "# sample_output:sample_inputをsample_hiddenの状態でencoderに突っ込んだ場合の出力\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoderの設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    #vocab_size: 辞書サイズ(出力側はまた違うサイズ)\n",
    "    #embedding_dim: 隠れ層サイズ\n",
    "    #dec_units: decodeのユニット数\n",
    "    #batch_sz: バッチサイズ\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # アテンションのため\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        #　まず、decoderの隠れとencorderのアウトプットからアテンション計算\n",
    "        # context_vector, attention_weightsを得る\n",
    "        # enc_output の shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # 埋め込み層を通過したあとの x の shape  == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 結合後の x の shape == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # GRUへは直列で渡す\n",
    "        # tf.concat : Negative axis are interpreted as counting from the end of the rank, i.e., axis + rank(values)-th dimension.\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # 結合したベクトルを GRU 層に渡す\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オプティマイザと損失関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チェックポイント（オブジェクトベースの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練\n",
    "- \n",
    "- エンコーダーの出力とエンコーダーの隠れ状態、そしてデコーダーの入力（これが 開始トークン）がデコーダーに渡される\n",
    "- デコーダーは 予測値 と デコーダーの隠れ状態 を返す\n",
    "- つぎにデコーダーの隠れ状態がモデルに戻され、予測値が損失関数の計算に使用される\n",
    "- デコーダーへの次の入力を決定するために Teacher Forcing が使用される\n",
    "- Teacher Forcing は、正解単語 をデコーダーの 次の入力 として使用するテクニックである\n",
    "- 最後に勾配を計算し、それをオプティマイザに与えて誤差逆伝播を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    # デコレータはtf.sessionのようなもの\n",
    "    loss = 0\n",
    "\n",
    "    #GradientTape とは勾配を求めるためのクラス\n",
    "    #内部でtapeで記録された変数は自動的に\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #入力をエンコーダーに通すと、エンコーダー出力とエンコーダーの隠れ状態 が返される\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        #状態の共有\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher Forcing - 正解値を次の入力として供給\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # Teacher Forcing を使用\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    #tf.keras.Model.trainable_variablesを呼び出すことで、そのネットワークに含まれる重みをリストとして得られる\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    #得られた変数について、微分→更新\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6384\n",
      "Epoch 1 Batch 100 Loss 2.2382\n",
      "Epoch 1 Batch 200 Loss 1.8553\n",
      "Epoch 1 Batch 300 Loss 1.6820\n",
      "Epoch 1 Loss 2.0010\n",
      "Time taken for 1 epoch 652.7817220687866 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.6052\n",
      "Epoch 2 Batch 100 Loss 1.4589\n",
      "Epoch 2 Batch 200 Loss 1.3249\n",
      "Epoch 2 Batch 300 Loss 1.2689\n",
      "Epoch 2 Loss 1.3451\n",
      "Time taken for 1 epoch 695.7474308013916 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.0709\n",
      "Epoch 3 Batch 100 Loss 0.9688\n",
      "Epoch 3 Batch 200 Loss 0.8728\n",
      "Epoch 3 Batch 300 Loss 0.9290\n",
      "Epoch 3 Loss 0.9185\n",
      "Time taken for 1 epoch 687.9900529384613 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.5766\n",
      "Epoch 4 Batch 100 Loss 0.6308\n",
      "Epoch 4 Batch 200 Loss 0.6592\n",
      "Epoch 4 Batch 300 Loss 0.6458\n",
      "Epoch 4 Loss 0.6059\n",
      "Time taken for 1 epoch 629.6567442417145 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3796\n",
      "Epoch 5 Batch 100 Loss 0.3520\n",
      "Epoch 5 Batch 200 Loss 0.3838\n",
      "Epoch 5 Batch 300 Loss 0.4244\n",
      "Epoch 5 Loss 0.4039\n",
      "Time taken for 1 epoch 656.3540561199188 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.2757\n",
      "Epoch 6 Batch 100 Loss 0.3140\n",
      "Epoch 6 Batch 200 Loss 0.2150\n",
      "Epoch 6 Batch 300 Loss 0.2938\n",
      "Epoch 6 Loss 0.2772\n",
      "Time taken for 1 epoch 665.2811560630798 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1577\n",
      "Epoch 7 Batch 100 Loss 0.2051\n",
      "Epoch 7 Batch 200 Loss 0.2094\n",
      "Epoch 7 Batch 300 Loss 0.2276\n",
      "Epoch 7 Loss 0.1966\n",
      "Time taken for 1 epoch 636.6115672588348 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1271\n",
      "Epoch 8 Batch 100 Loss 0.1796\n",
      "Epoch 8 Batch 200 Loss 0.1252\n",
      "Epoch 8 Batch 300 Loss 0.1904\n",
      "Epoch 8 Loss 0.1461\n",
      "Time taken for 1 epoch 634.8577151298523 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1230\n",
      "Epoch 9 Batch 100 Loss 0.0853\n",
      "Epoch 9 Batch 200 Loss 0.1604\n",
      "Epoch 9 Batch 300 Loss 0.1388\n",
      "Epoch 9 Loss 0.1134\n",
      "Time taken for 1 epoch 651.6503577232361 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0823\n",
      "Epoch 10 Batch 100 Loss 0.0663\n",
      "Epoch 10 Batch 200 Loss 0.0703\n",
      "Epoch 10 Batch 300 Loss 0.0799\n",
      "Epoch 10 Loss 0.0939\n",
      "Time taken for 1 epoch 654.1339030265808 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()))\n",
    "            \n",
    "    # 2 エポックごとにモデル（のチェックポイント）を保存\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # 後ほどプロットするためにアテンションの重みを保存\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # 予測された ID がモデルに戻される\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アテンションの重みをプロットする関数\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x134d096d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint_dir の中の最後のチェックポイントを復元\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: voted voted voted voted voted voted voted voted voted voted voted \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
